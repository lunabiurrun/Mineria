{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f4189bae933e3dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Árboles de decisión\n",
    "\n",
    "- [Árboles de decisión para clasificación](#Árboles-de-decisión-para-clasificación)\n",
    "- [Extracción de variables utilizando árboles de decisión](#Extracción-de-variables-utilizando-árboles-de-decisión)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b28d67ab55cbd64a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Árboles de decisión para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39e6bbc77fef3f81",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Scikit-learn nos ofrece implementados dos de los algoritmos más utilizados para generar árboles de decisión: C4.5 y CART. \n",
    "\n",
    "En primer lugar vamos a aprender a utilizar el paquete de la librería Scikit-learn (sklearn) que corresponde al paradigma de los árboles de decisión. Estos clasificadores están almacenados en la librería *tree*, por tanto en primer lugar debemos importarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb2d4c779c52f95a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Se importa la librería de árboles de decisión\n",
    "# BEGIN SOLUTION\n",
    "from sklearn import tree\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d36165f97a45f08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Dentro de dicha librería está la clase específica que permite crear árboles de decisión para resolver problemas de clasificación que se llama [*DecisionTreeClassifier*](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "\n",
    "La clase *DecisionTreeClassifier* tiene varios parámetros que se pueden utilizar para determinar el árbol de decisión a utilizar (C4.5 o CART) así como varios de sus híper-parámetros que determinarán su posterior comportamiento. La llamada al constructor del árbol de decisión es la siguiente:\n",
    "\n",
    "    clasificador = tree.DecisionTreeClassifier(criterion=tipoMedidaImpurezaNodo, min_samples_split=numeroMinimoEjemplosRama, min_samples_leaf= numeroMinimoEjemplosHoja, max_depth=maximaProfundidad, ccp_alpha=umbralAlfaEficaz, random_state=semilla)\n",
    "\n",
    "Los valores de los híper-parámetros que determinan el comportamiento del clasificador (parte derecha de la asignación de los parámetros criterion, min_samples_split y min_samples_leaf) son los siguientes:\n",
    "* tipoMedidaImpurezaNodo: tipo de medida de la impureza del nodo\n",
    "    * ‘gini’: indice GINI, árbol de decisión CART\n",
    "    * ‘entropy’: ratio de ganancia de información, árbol de decisión C4.5\n",
    "* numeroMinimoEjemplosRama: este híper-parámetro determina el número mínimo de ejemplos que debe tener un nodo interno para dividirlo más. Admite valores enteros o reales (entre 0 y 1): en caso de valores enteros determinan el valor exacto a aplicar y en caso de números reales representa el porcentaje de ejemplos a utilizar como umbral. El valor por defecto es 2 e implica realizar una expansión total del árbol.\n",
    "* numeroMinimoEjemplosHoja: este híper-parámetro determina el número mínimo de ejemplos que debe tener cualquier nodo hijo generado a partir del nodo a dividir. Si alguno no llega a este valor no se divide el nodo padre. Admite valores enteros o reales (entre 0 y 1): en caso de valores enteros determinan el valor exacto a aplicar y en caso de números reales representa el porcentaje de ejemplos a utilizar como umbral. El valor por defecto es 1 e implica realizar una expansión total del árbol.\n",
    "* maximaProfundidad: este híper-parámetro permite valores enteros y determina la profundidad máxima permitida en el árbol. Si la profunidad es mayor que este valor no se divide más el árbol. Por tanto, el arbol generado será más simple. El valor por defecto es *None*, que implica expandir el árbol sin restricciones de profundidad.\n",
    "* umbralAlfaEficaz: valor del umbral utilizado para detener el proceso de poda. Por defecto es 0.0 que implica no realizar poda.\n",
    "* semilla: valor de la semilla para generar los números aleatorios (se utiliza para seleccionar la mejor variable/umbral en caso de empates).\n",
    "\n",
    "NOTA: para resolver problemas de regresión la clase correspondiente a los árboles de decisión se llama [*DecisionTreeRegressor*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) y sus híper-parámetros son análogos a los explicados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56008b0f280f5aba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Los árboles de decisión tienden a sobre-entrenar y, por tanto, los árboles generados pueden ser muy complicados para las características de los ejemplos de entrenamiento. Para tratar de no realizar árboles muy complicados (ajustados a los datos de entrenamiento) podemos utilizar las técnicas de pre-poda que están determinadas mediante los valores de los híper-parámetros (mencionados anteriormente):\n",
    "* *min_samples_split*: determina el número mínimo de ejemplos que debe tener un nodo interno para dividirlo más.\n",
    "    * Cuanto mayor sea el valor antes se parará de dividir y el árbol será más simple (normalmente implica obtener un accuracy menor para train).\n",
    "    * Cuanto más bajo sea este valor más fácil será que los nodos se dividan y más se ajustará el árbol a los datos de entrenamiento (normalmente implica obtener un accuracy mayor para train).\n",
    "* *min_samples_leaf*: determina el número mínimo de ejemplos que debe tener un nodo para generar una hoja. Si no llega a este valor no se genera el nodo hoja.\n",
    "    * Cuanto mayor sea este valor más difícil será generar nodos hoja y, por tanto, el árbol generado será más simple. Suele implicar perder accuracy en train.\n",
    "* *max_depth*: determina la profundidad máxima permitida en el árbol. Si la profunidad es mayor que este valor no se divide más el árbol.\n",
    "    * Cuanto mayor sea este valor más profundo será el árbol de decisión y, por tanto, más riesgo de sobre-entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31ff1b22d0f570e0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Para mostrar visualmente los árboles de decisión aprendidos Scikit-learn nos ofrece una función (del paquete *tree*) que puede utilizarse para mostrarlos gráficamente llamada [*plot_tree*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). Su llamada y los argumentos que recibe son los siguientes:\n",
    "\n",
    "    tree.plot_tree(arbol, feature_names=nombresVariables, class_names=nombresClases, filled=True, rounded=True, node_ids=True)\n",
    "\n",
    "El significado de las variables es el siguiente:\n",
    "* arbol: el árbol de decisión a mostrar, el obtenido tras realizar el aprendizaje.\n",
    "* nombresVariables: nombres de las variables de entrada, en este caso iris.feature_names\n",
    "* nombresClases: nombres de las clases del problema, en este caso iris.target_names\n",
    "* Los argumentos (filled y rounded) hay que dejarlos como están en este ejemplo (True) para que el árbol de decisión tenga una visualización más agradable.\n",
    "* node_ids es un valor booleano que hace que se imprima el identificador de cada nodo (True) o no (False, valor por defecto).\n",
    "  \n",
    "NOTAS:\n",
    "* Los arcos que van a la izquierda cumplen la condición mostrada en el nodo del que parten y los arcos que van a la derecha no la cumplen.\n",
    "* El método [*savefig*](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) de *MatPlotLib* permite guardar el contenido de una figura en un archivo (*variableFigura.savefig(nombreFichero)*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce3daba1b1d42eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "También se puede obtener la representación en texto del árbol. Para ello, se debe utilizar el método [*export_text*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html) de la librería *tree* a la que se pasa como argumento de entrada el árbol a mostrar. Además, podemos guardar dicho texto en un fichero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a9ef8c15eac6b84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "La interpretabilidad global de los árboles de decisión está condicionada por dos factores:\n",
    "* *Número de hojas*: determina el número reglas y por tanto, cuantas más hojas será más difícil de interpretar globalmente el árbol obtenido. \n",
    "* *Número de condiciones para alcanzar una hoja*: cuantas más condiciones sean necesarias analizar para tomar la decisión más difícil será interpretarla.\n",
    "    * El número medio (o máximo) de condiciones para alcanzar todas las hojas del árbol da una idea global de este factor.\n",
    "    \n",
    "En Scikit-learn podemos ver algunos de los valores que caraterizan la interpretabilidad del árbol como:\n",
    "* El número de hojas del árbol de decisión, método *get_n_leaves* de la clase *DecisionTreeClassifier*.\n",
    "* La profundidad máxima del árbol de decisión, método *get_depth* de la clase *DecisionTreeClassifier*.\n",
    "* El número total de nodos del árbol de decisión, atributo *node_count* del atributo *tree_* del objeto de la clase *DecisionTreeClassifier* (*arbolDecision.tree_.node_count*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f90ea8d99e833c54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Además, podemos mostrar textualmente la ruta seguida por un ejemplo para realizar su predicción. Para ello, os facilito una función que lo hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eeeffb9c08499d10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Función para mostrar textualmente la ruta seguida para clasificar un ejemplo con un arbol\n",
    "def mostrarTextualmenteRutaClasificacion(arbol, ejemplo, nombresVariables):\n",
    "# arbol: arbol de decisión entrenado\n",
    "# ejemplo: ejemplo a clasificar y mostrar su ruta\n",
    "# nombresVariables: lista de los nombres de las variables del problema\n",
    "\n",
    "    # Obtenemos valores representativos de la estructura del árbol\n",
    "    atributos = arbol.tree_.feature\n",
    "    umbrales = arbol.tree_.threshold\n",
    "\n",
    "    # Se obtiene la ruta seguida para clasificar el ejemplo\n",
    "    ruta = arbol.decision_path(ejemplo)\n",
    "    # Se obtiene el identificador del nodo hoja usado para clasificar el ejemplo\n",
    "    id_hoja = arbol.apply(ejemplo)\n",
    "\n",
    "\n",
    "    # Se obtienen los indices de los nodos a través de los que pasa el ejemplo (id_ejemplo)\n",
    "    indices_nodos = ruta.indices[ruta.indptr[0]: ruta.indptr[0 + 1]]\n",
    "\n",
    "    print('Reglas usadas para predecir el ejemplo: {}\\n'.format(ejemplo[0]))\n",
    "    for id_nodo in indices_nodos:\n",
    "        # Pasamos al siguiente nodo si es un nodo hoja\n",
    "        if id_hoja[0] == id_nodo:\n",
    "            continue\n",
    "\n",
    "        # Comprobamos si el valor del ejemplo es menor que el valor usado para dividir el nodo (umbral) \n",
    "        if (ejemplo[0, atributos[id_nodo]] <= umbrales[id_nodo]):\n",
    "            desigualdad = \"<=\"\n",
    "        else:\n",
    "            desigualdad = \">\"\n",
    "        \n",
    "        print(\"Nodo decisión #{nodo} : (Ejemplo[{atributo}] = {valor}) \"\n",
    "              \"{desiguald} {umbral})\".format(\n",
    "                  nodo=id_nodo,\n",
    "                  atributo=nombresVariables[atributos[id_nodo]],\n",
    "                  valor=ejemplo[0][atributos[id_nodo]],\n",
    "                  desiguald=desigualdad,\n",
    "                  umbral=umbrales[id_nodo]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4577a70751f8c21f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Poda de árboles de decisión\n",
    "\n",
    "Existen diversas técnicas de poda, la que aplica Scikit-learn es la que obtiene el sub-árbol, $T$, que obtiene la mejor (mínima) relación entre coste y complejidad tal y como hemos visto en teoría.\n",
    "\n",
    "La medida de coste es tradicionalmente el ratio de error de los nodos hoja. Sin embargo, en Scikit-learn se utiliza la media ponderada de la impureza de los nodos hoja como valor del coste.\n",
    "\n",
    "Para tener una idea de los valores que pueden ser útiles para el híper-parámetro que determina la condición de parada de la poda, podemos utilizar el método [*cost_complexity_pruning_path*](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html) de la clase *DecisionTreeClassifier* al que se deben pasar los datos de entrenamiento. Este método devuelve todos los $\\alpha$ eficaces que se utilizarían hasta podar el árbol entero (solo quedaría el nodo raíz etiquetado con la clase mayoritaria del problema) así como la impureza del árbol podado para cada $\\alpha$ eficaz. \n",
    "\n",
    "NOTA: si el número de $\\alpha$ eficaces es muy grande el tiempo de cómputo de las gráficas puede ser muy grande.\n",
    "NOTA 2: a partir de dichos $\\alpha$ eficaces se puede establecer el valor del híper-parámetro *ccp_alpha* para que se realice la poda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c84331d64d33b74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Extracción de variables utilizando árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcd6b12258adefdd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Por último vamos a utilizar los árboles de decisión como método de selección de variables. Para ello, la librería de selección de variables (*feature_selection*) de Scikit-learn nos ofrece la clase [*SelectFromModel*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) que nos facilita la tarea. La llamada al constructor de esta clase es la siguiente:\n",
    "\n",
    "    modelo = SelectFromModel(estimator, threshold=valorUmbral)\n",
    "\n",
    "Los argmnetos son los siguientes: \n",
    "* estimator: modelo a partir del cual obtener las variables\n",
    "    * Si la clase del modelo tiene un atributo que determine la importancia de las variables se puede aplicar esta clase\n",
    "* threshold=valorUmbral: valor del umbral para seleccionar las variables, puede ser\n",
    "    * Valor numérico: valor asignado al umbral (float)\n",
    "    * String: método a utilizar, por ejemplo la mediana de los valores de la importancia de todas las variables ('median' o 'mean'). También puede ser un string que indique que el valor sea un tanto por ciento mayor que la media por ejemplo ('1.25*mean').\n",
    "\n",
    "Todas las variables que tengan una importancia mayor que el umbral establecido o generado son seleccionadas.\n",
    "\n",
    "Una vez ejecutada la llamada al constructor hemos generado un objeto (almacenado en la variable model en el código de la celda posterior) que debemos entrenar (como si fuera un clasificador) puesto que el parámetro *prefit* (indice si el modelo ha sido entrenado previamente) por defecto es *False* (si fuera *True* no haría falta entrenar el objeto de *SelectFromModel*). Para ello llamamos al método *fit*. Una vez entrenado, podemos comprobar las variables seleccionadas ejecutando el método *get_support()* que devuelve una lista de booleanos con tantos elementos como variables y que tendrá valor True en las posiciones de las variables seleccionadas. \n",
    "\n",
    "Además, este objeto puede ser utilizado para transformar los datos y realizar la selección de variables. Para ello se utiliza el método *transform* del objeto almacenado en la variable model (*model.transform(datosATransformar)*). Este método nos devuelve los datos de entrada pero solamente de las variables seleccionadas (las que estén a True al ejecutar el método *get_support()*)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
